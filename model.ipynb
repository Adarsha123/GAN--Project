{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c867685c-bbde-419f-895b-53749beeae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c11139-b669-49d6-80aa-2bc2a6771596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualBatchNorm1d(Module):\n",
    "    \"\"\"\n",
    "    Module for Virtual Batch Normalization.\n",
    "\n",
    "    Implementation borrowed and modified from Rafael_Valle's code + help of SimonW from this discussion thread:\n",
    "    https://discuss.pytorch.org/t/parameter-grad-of-conv-weight-is-none-after-virtual-batch-normalization/9036\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # batch statistics\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps  # epsilon\n",
    "        # define gamma and beta parameters\n",
    "        self.gamma = Parameter(torch.normal(mean=1.0, std=0.02, size=(1, num_features, 1)))\n",
    "        self.beta = Parameter(torch.zeros(1, num_features, 1))\n",
    "\n",
    "    def get_stats(self, x):\n",
    "        \"\"\"\n",
    "        Calculates mean and mean square for given batch x.\n",
    "        Args:\n",
    "            x: tensor containing batch of activations\n",
    "        Returns:\n",
    "            mean: mean tensor over features\n",
    "            mean_sq: squared mean tensor over features\n",
    "        \"\"\"\n",
    "        mean = x.mean(2, keepdim=True).mean(0, keepdim=True)\n",
    "        mean_sq = (x ** 2).mean(2, keepdim=True).mean(0, keepdim=True)\n",
    "        return mean, mean_sq\n",
    "\n",
    "    def forward(self, x, ref_mean, ref_mean_sq):\n",
    "        \"\"\"\n",
    "        Forward pass of virtual batch normalization.\n",
    "        Virtual batch normalization require two forward passes\n",
    "        for reference batch and train batch, respectively.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor\n",
    "            ref_mean: reference mean tensor over features\n",
    "            ref_mean_sq: reference squared mean tensor over features\n",
    "        Result:\n",
    "            x: normalized batch tensor\n",
    "            ref_mean: reference mean tensor over features\n",
    "            ref_mean_sq: reference squared mean tensor over features\n",
    "        \"\"\"\n",
    "        mean, mean_sq = self.get_stats(x)\n",
    "        if ref_mean is None or ref_mean_sq is None:\n",
    "            # reference mode - works just like batch norm\n",
    "            mean = mean.clone().detach()\n",
    "            mean_sq = mean_sq.clone().detach()\n",
    "            out = self.normalize(x, mean, mean_sq)\n",
    "        else:\n",
    "            # calculate new mean and mean_sq\n",
    "            batch_size = x.size(0)\n",
    "            new_coeff = 1. / (batch_size + 1.)\n",
    "            old_coeff = 1. - new_coeff\n",
    "            mean = new_coeff * mean + old_coeff * ref_mean\n",
    "            mean_sq = new_coeff * mean_sq + old_coeff * ref_mean_sq\n",
    "            out = self.normalize(x, mean, mean_sq)\n",
    "        return out, mean, mean_sq\n",
    "\n",
    "    def normalize(self, x, mean, mean_sq):\n",
    "        \"\"\"\n",
    "        Normalize tensor x given the statistics.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor\n",
    "            mean: mean over features\n",
    "            mean_sq: squared means over features\n",
    "\n",
    "        Result:\n",
    "            x: normalized batch tensor\n",
    "        \"\"\"\n",
    "        assert mean_sq is not None\n",
    "        assert mean is not None\n",
    "        assert len(x.size()) == 3  # specific for 1d VBN\n",
    "        if mean.size(1) != self.num_features:\n",
    "            raise Exception('Mean tensor size not equal to number of features : given {}, expected {}'\n",
    "                            .format(mean.size(1), self.num_features))\n",
    "        if mean_sq.size(1) != self.num_features:\n",
    "            raise Exception('Squared mean tensor size not equal to number of features : given {}, expected {}'\n",
    "                            .format(mean_sq.size(1), self.num_features))\n",
    "\n",
    "        std = torch.sqrt(self.eps + mean_sq - mean ** 2)\n",
    "        x = x - mean\n",
    "        x = x / std\n",
    "        x = x * self.gamma\n",
    "        x = x + self.beta\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('{name}(num_features={num_features}, eps={eps}'\n",
    "                .format(name=self.__class__.__name__, **self.__dict__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab70b05-cc3b-40d4-83d6-15b2793f7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"G\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # encoder gets a noisy signal as input [B x 2 x 16384]\n",
    "        self.enc1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=31, stride=2, padding=15)  # [B x 16 x 8192] []\n",
    "        self.enc1_nl = nn.PReLU()\n",
    "        self.enc2 = nn.Conv1d(16, 32, 32, 2, 15)  # [B x 32 x 4096]\n",
    "        self.enc2_nl = nn.PReLU()\n",
    "        self.enc3 = nn.Conv1d(32, 32, 32, 2, 15)  # [B x 32 x 2048]\n",
    "        self.enc3_nl = nn.PReLU()\n",
    "        self.enc4 = nn.Conv1d(32, 64, 32, 2, 15)  # [B x 64 x 1024]\n",
    "        self.enc4_nl = nn.PReLU()\n",
    "        self.enc5 = nn.Conv1d(64, 64, 32, 2, 15)  # [B x 64 x 512]\n",
    "        self.enc5_nl = nn.PReLU()\n",
    "        self.enc6 = nn.Conv1d(64, 128, 32, 2, 15)  # [B x 128 x 256]\n",
    "        self.enc6_nl = nn.PReLU()\n",
    "        self.enc7 = nn.Conv1d(128, 256, 32, 2, 15)  # [B x 128 x 128]\n",
    "        self.enc7_nl = nn.PReLU()\n",
    "        \n",
    "\n",
    "        # decoder generates an enhanced signal\n",
    "        # each decoder output are concatenated with homologous encoder output,\n",
    "        # so the feature map sizes are doubled\n",
    "     \n",
    "        self.dec6 = nn.ConvTranspose1d(512, 128, 64, 2, 31)  # [B x 128 x 256]\n",
    "        self.dec6_nl = nn.PReLU()\n",
    "        self.dec5 = nn.ConvTranspose1d(256, 64, 64, 2, 31)  # [B x 64 x 512]\n",
    "        self.dec5_nl = nn.PReLU()\n",
    "        self.dec4 = nn.ConvTranspose1d(128, 64, 64, 2, 31)  # [B x 64 x 1024]\n",
    "        self.dec4_nl = nn.PReLU()\n",
    "        self.dec3 = nn.ConvTranspose1d(128, 32, 64, 2, 31)  # [B x 32 x 2048]\n",
    "        self.dec3_nl = nn.PReLU()\n",
    "        self.dec2 = nn.ConvTranspose1d(64, 32, 64, 2, 31)  # [B x 32 x 4096]\n",
    "        self.dec2_nl = nn.PReLU()\n",
    "        self.dec1 = nn.ConvTranspose1d(64, 16, 64, 2, 31)  # [B x 16 x 8192]\n",
    "        self.dec1_nl = nn.PReLU()\n",
    "        self.dec_final = nn.ConvTranspose1d(32, 1, 64, 2, 31)  # [B x 1 x 16384]\n",
    "        self.dec_tanh = nn.Tanh()\n",
    "        \n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for convolution layers using Xavier initialization.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "    def forward(self, x, va, z):\n",
    "        \"\"\"\n",
    "        Forward pass of generator.\n",
    "\n",
    "        Args:\n",
    "            x: input batch (noisy signal)\n",
    "            va: va input\n",
    "            z: latent vector\n",
    "        \"\"\"\n",
    "        input = torch.cat((x,va),dim=1)\n",
    "        e1 = self.enc1(input)\n",
    "        e2 = self.enc2(self.enc1_nl(e1))\n",
    "        e3 = self.enc3(self.enc2_nl(e2))\n",
    "        e4 = self.enc4(self.enc3_nl(e3))\n",
    "        e5 = self.enc5(self.enc4_nl(e4))\n",
    "        e6 = self.enc6(self.enc5_nl(e5))\n",
    "        e7 = self.enc7(self.enc6_nl(e6))\n",
    "        c=self.enc7_nl(e7) #[64 * 128 * 7/8]\n",
    "\n",
    "\n",
    "        \n",
    "        encoded = torch.cat((c, z), dim=1)\n",
    "       \n",
    "     \n",
    "        d6 = self.dec6(encoded)\n",
    "        d6_c = self.dec6_nl(torch.cat((d6, e6), dim=1))\n",
    "        d5 = self.dec5(d6_c)\n",
    "        d5_c = self.dec5_nl(torch.cat((d5, e5), dim=1))\n",
    "        d4 = self.dec4(d5_c)\n",
    "        d4_c = self.dec4_nl(torch.cat((d4, e4), dim=1))\n",
    "        d3 = self.dec3(d4_c)\n",
    "        d3_c = self.dec3_nl(torch.cat((d3, e3), dim=1))\n",
    "        d2 = self.dec2(d3_c)\n",
    "        d2_c = self.dec2_nl(torch.cat((d2, e2), dim=1))\n",
    "        d1 = self.dec1(d2_c)\n",
    "        d1_c = self.dec1_nl(torch.cat((d1, e1), dim=1))\n",
    "        out = self.dec_tanh(self.dec_final(d1_c))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c1ca8a-ef4c-4bb5-943b-155f2db58065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"D\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # D gets a noisy signal and clear signal as input [B x 2 x 16384]\n",
    "        negative_slope = 0.03\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=31, stride=2, padding=15)  # [B x 32 x 8192]\n",
    "        self.vbn1 = VirtualBatchNorm1d(32)\n",
    "        self.lrelu1 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 31, 2, 15)  # [B x 64 x 4096]\n",
    "        self.vbn2 = VirtualBatchNorm1d(64)\n",
    "        self.lrelu2 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv3 = nn.Conv1d(64, 64, 31, 2, 15)  # [B x 64 x 2048]\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.vbn3 = VirtualBatchNorm1d(64)\n",
    "        self.lrelu3 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv4 = nn.Conv1d(64, 128, 31, 2, 15)  # [B x 128 x 1024]\n",
    "        self.vbn4 = VirtualBatchNorm1d(128)\n",
    "        self.lrelu4 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv5 = nn.Conv1d(128, 128, 31, 2, 15)  # [B x 128 x 512]\n",
    "        self.vbn5 = VirtualBatchNorm1d(128)\n",
    "        self.lrelu5 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv6 = nn.Conv1d(128, 256, 31, 2, 15)  # [B x 256 x 256]\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.vbn6 = VirtualBatchNorm1d(256)\n",
    "        self.lrelu6 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv7 = nn.Conv1d(256, 256, 31, 2, 15)  # [B x 256 x 128]\n",
    "        self.vbn7 = VirtualBatchNorm1d(256)\n",
    "        self.lrelu7 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv8 = nn.Conv1d(256, 512, 31, 2, 15)  # [B x 512 x 64]\n",
    "        self.vbn8 = VirtualBatchNorm1d(512)\n",
    "        self.lrelu8 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv9 = nn.Conv1d(512, 512, 31, 2, 15)  # [B x 512 x 32]\n",
    "        self.dropout3 = nn.Dropout()\n",
    "        self.vbn9 = VirtualBatchNorm1d(512)\n",
    "        self.lrelu9 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv10 = nn.Conv1d(512, 1024, 31, 2, 15)  # [B x 1024 x 16]\n",
    "        self.vbn10 = VirtualBatchNorm1d(1024)\n",
    "        self.lrelu10 = nn.LeakyReLU(negative_slope)\n",
    "        self.conv11 = nn.Conv1d(1024, 2048, 31, 2, 15)  # [B x 2048 x 8]\n",
    "        self.vbn11 = VirtualBatchNorm1d(2048)\n",
    "        self.lrelu11 = nn.LeakyReLU(negative_slope)\n",
    "        # 1x1 size kernel for dimension and parameter reduction\n",
    "        self.conv_final = nn.Conv1d(2048, 1, kernel_size=1, stride=1,padding=0)  # [B x 1 x 8]\n",
    "        self.lrelu_final = nn.LeakyReLU(negative_slope)\n",
    "        self.fully_connected = nn.Linear(in_features=32, out_features=1)  # [B x 1]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for convolution layers using Xavier initialization.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "    def forward(self, x, ref_x):\n",
    "        \"\"\"\n",
    "        Forward pass of discriminator.\n",
    "\n",
    "        Args:\n",
    "            x: input batch (signal)\n",
    "            ref_x: reference input batch for virtual batch norm\n",
    "        \"\"\"\n",
    "        # reference pass\n",
    "        # print(\"first shape of ref_x= \",ref_x.shape)\n",
    "        ref_x = self.conv1(ref_x)\n",
    "        # print(\"ref_x conv1=  \",ref_x.shape)\n",
    "        ref_x, mean1, meansq1 = self.vbn1(ref_x, None, None)\n",
    "        # print(\"ref_x conv1 vbn1=  \",ref_x.shape)\n",
    "        ref_x = self.lrelu1(ref_x)\n",
    "        ref_x = self.conv2(ref_x)\n",
    "        ref_x, mean2, meansq2 = self.vbn2(ref_x, None, None)\n",
    "        ref_x = self.lrelu2(ref_x)\n",
    "        ref_x = self.conv3(ref_x)\n",
    "        ref_x = self.dropout1(ref_x)\n",
    "        ref_x, mean3, meansq3 = self.vbn3(ref_x, None, None)\n",
    "        ref_x = self.lrelu3(ref_x)\n",
    "        ref_x = self.conv4(ref_x)\n",
    "        ref_x, mean4, meansq4 = self.vbn4(ref_x, None, None)\n",
    "        ref_x = self.lrelu4(ref_x)\n",
    "        ref_x = self.conv5(ref_x)\n",
    "        ref_x, mean5, meansq5 = self.vbn5(ref_x, None, None)\n",
    "        ref_x = self.lrelu5(ref_x)\n",
    "        ref_x = self.conv6(ref_x)\n",
    "        ref_x = self.dropout2(ref_x)\n",
    "        ref_x, mean6, meansq6 = self.vbn6(ref_x, None, None)\n",
    "        ref_x = self.lrelu6(ref_x)\n",
    "        ref_x = self.conv7(ref_x)\n",
    "        ref_x, mean7, meansq7 = self.vbn7(ref_x, None, None)\n",
    "        ref_x = self.lrelu7(ref_x)\n",
    "        ref_x = self.conv8(ref_x)\n",
    "        ref_x, mean8, meansq8 = self.vbn8(ref_x, None, None)\n",
    "        ref_x = self.lrelu8(ref_x)\n",
    "        ref_x = self.conv9(ref_x)\n",
    "        ref_x = self.dropout3(ref_x)\n",
    "        ref_x, mean9, meansq9 = self.vbn9(ref_x, None, None)\n",
    "        ref_x = self.lrelu9(ref_x)\n",
    "        ref_x = self.conv10(ref_x)\n",
    "        ref_x, mean10, meansq10 = self.vbn10(ref_x, None, None)\n",
    "        ref_x = self.lrelu10(ref_x)\n",
    "        ref_x = self.conv11(ref_x)\n",
    "        ref_x, mean11, meansq11 = self.vbn11(ref_x, None, None)\n",
    " \n",
    "        # further pass no longer needed\n",
    "     \n",
    "        # train pass\n",
    "        x = x.to(self.device)\n",
    "        x = self.conv1(x)\n",
    "        x, _, _ = self.vbn1(x, mean1, meansq1)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x, _, _ = self.vbn2(x, mean2, meansq2)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _, _ = self.vbn3(x, mean3, meansq3)\n",
    "        x = self.lrelu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x, _, _ = self.vbn4(x, mean4, meansq4)\n",
    "        x = self.lrelu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x, _, _ = self.vbn5(x, mean5, meansq5)\n",
    "        x = self.lrelu5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _, _ = self.vbn6(x, mean6, meansq6)\n",
    "        x = self.lrelu6(x)\n",
    "        x = self.conv7(x)\n",
    "        x, _, _ = self.vbn7(x, mean7, meansq7)\n",
    "        x = self.lrelu7(x)\n",
    "        x = self.conv8(x)\n",
    "        x, _, _ = self.vbn8(x, mean8, meansq8)\n",
    "        x = self.lrelu8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _, _ = self.vbn9(x, mean9, meansq9)\n",
    "        x = self.lrelu9(x)\n",
    "        x = self.conv10(x)\n",
    "        x, _, _ = self.vbn10(x, mean10, meansq10)\n",
    "        x = self.lrelu10(x)\n",
    "        x = self.conv11(x)\n",
    "        x, _, _ = self.vbn11(x, mean11, meansq11)\n",
    "        x = self.lrelu11(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = self.lrelu_final(x)\n",
    "        x = torch.squeeze(x)\n",
    "        infeatures = x.shape[-1]\n",
    "        self.fully_connected=nn.Linear(infeatures,1).to(x.device)\n",
    "     \n",
    "        x = self.fully_connected(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26adbb-f55b-4d81-8342-0011668d3718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edee903-fe81-40ed-84ea-ae3322957e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
